{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using K-Means and Elbow Method\n",
   "id": "c41e74b8df4c0217"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "RANDOM_STATE = 42"
   ],
   "id": "247e722db848553c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1) Load data\n",
   "id": "fb06bbc244d6a740"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRAIN_PATH = os.environ.get(\"TRAIN_PATH\", \"MiNDAT.csv\")\n",
    "TEST_PATH = os.environ.get(\"TEST_PATH\", \"MiNDAT_UNK.csv\")\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")"
   ],
   "id": "7aa9cebe8463032a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2) Setting identifiers\n",
   "id": "79b27548e3fc16de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TARGET_COL = \"CORRUCYSTIC_DENSITY\"\n",
    "ID_COL = \"LOCAL_IDENTIFIER\"\n",
    "\n",
    "test_ids = test[ID_COL].copy()"
   ],
   "id": "de1e98e659a660a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3) Select numeric features, impute medians, and robust-scale\n",
   "id": "f9c64f578a2a3815"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use only numeric columns for modeling\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Ensure the target is in numeric columns and remove it from features\n",
    "if TARGET_COL in numeric_cols:\n",
    "    numeric_cols.remove(TARGET_COL)\n",
    "\n",
    "# Intersection with test (to avoid train-only numeric columns)\n",
    "numeric_cols = [c for c in numeric_cols if c in test.columns]\n",
    "\n",
    "# Separate X/y\n",
    "X_train_raw = train[numeric_cols].copy()\n",
    "y_train = train[TARGET_COL].copy()\n",
    "X_test_raw = test[numeric_cols].copy()\n",
    "\n",
    "# Fill NaNs with per-column medians fit on train\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train_raw), columns=numeric_cols, index=X_train_raw.index\n",
    ")\n",
    "X_test_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_test_raw), columns=numeric_cols, index=X_test_raw.index\n",
    ")\n",
    "\n",
    "# Robust scaling helps with outliers\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_imputed),\n",
    "    columns=numeric_cols,\n",
    "    index=X_train_imputed.index,\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_imputed), columns=numeric_cols, index=X_test_imputed.index\n",
    ")"
   ],
   "id": "84db2c61ad4fd1dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4) Feature filtering by correlation with the target\n",
   "id": "f55cf091b021c31a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute correlations\n",
    "cors = {}\n",
    "for col in numeric_cols:\n",
    "    try:\n",
    "        corr = np.corrcoef(X_train_imputed[col], y_train)[0, 1]\n",
    "    except Exception:\n",
    "        corr = np.nan\n",
    "    cors[col] = corr\n",
    "\n",
    "corr_series = (\n",
    "    pd.Series(cors).dropna().sort_values(key=lambda s: s.abs(), ascending=False)\n",
    ")\n",
    "\n",
    "# Keep top N features or all if there are few\n",
    "TOP_N = min(\n",
    "    50, max(10, int(0.8 * len(corr_series)))\n",
    ")  # keep up to 50, at least 10, or 80% of available\n",
    "selected_features = corr_series.head(TOP_N).index.tolist()\n",
    "\n",
    "print(\"Top correlations (sign kept, sorted by |corr|):\")\n",
    "display(corr_series.head(20))\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features for clustering.\")\n",
    "\n",
    "Xtr = X_train_scaled[selected_features].copy()\n",
    "Xte = X_test_scaled[selected_features].copy()"
   ],
   "id": "f58ead71b73dba12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5) Preparing X for clustering",
   "id": "4b33874937ec7eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _prepare_X_for_clustering(X):\n",
    "    if X is None:\n",
    "        raise ValueError(\"X is None. Provide a non-empty array or DataFrame.\")\n",
    "\n",
    "    # Convert Series/list to numpy\n",
    "    if isinstance(X, pd.Series):\n",
    "        X = X.to_frame()\n",
    "    elif isinstance(X, list):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "    # If DataFrame, keep only numeric columns\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_num = X.select_dtypes(include=[np.number])\n",
    "        if X_num.shape[1] == 0:\n",
    "            raise ValueError(\"X has no numeric columns after dtype filtering.\")\n",
    "        # Drop rows that are completely NaN\n",
    "        X_num = X_num.dropna(how=\"all\")\n",
    "        X_arr = X_num.to_numpy()\n",
    "    else:\n",
    "        X_arr = np.asarray(X)\n",
    "\n",
    "    # Ensure at least 1 dimension\n",
    "    if X_arr.ndim == 0:\n",
    "        raise ValueError(\n",
    "            \"X is a scalar; expected array-like with at least 1 dimension.\"\n",
    "        )\n",
    "\n",
    "    # If 1D, make it 2D\n",
    "    if X_arr.ndim == 1:\n",
    "        X_arr = X_arr.reshape(-1, 1)\n",
    "\n",
    "    # Drop rows that are fully NaN (if any)\n",
    "    if np.isnan(X_arr).any():\n",
    "        # Keep rows that have at least one non-NaN\n",
    "        mask = ~np.all(np.isnan(X_arr), axis=1)\n",
    "        X_arr = X_arr[mask]\n",
    "\n",
    "    n_samples, n_features = X_arr.shape\n",
    "    if n_samples == 0:\n",
    "        raise ValueError(\"X has 0 rows after cleaning. Provide non-empty data.\")\n",
    "    if n_features == 0:\n",
    "        raise ValueError(\"X has 0 features after cleaning. Provide numeric features.\")\n",
    "\n",
    "    # Replace any remaining NaNs with column means (or raise if you prefer strict)\n",
    "    if np.isnan(X_arr).any():\n",
    "        col_means = np.nanmean(X_arr, axis=0)\n",
    "        # For columns that are entirely NaN, np.nanmean yields NaN; handle those:\n",
    "        nan_cols = np.isnan(col_means)\n",
    "        if np.any(nan_cols):\n",
    "            # Drop entirely-NaN columns safely:\n",
    "            keep_cols = ~nan_cols\n",
    "            X_arr = X_arr[:, keep_cols]\n",
    "            if X_arr.shape[1] == 0:\n",
    "                raise ValueError(\"All features are NaN; cannot proceed.\")\n",
    "            col_means = np.nanmean(X_arr, axis=0)\n",
    "        inds = np.where(np.isnan(X_arr))\n",
    "        X_arr[inds] = np.take(col_means, inds[1])\n",
    "\n",
    "    return X_arr"
   ],
   "id": "34bbc131ec29f15a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6) Elbow method to choose K",
   "id": "777667c7d50dca14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_elbow_inertia(X, k_min=1, k_max=12, random_state=RANDOM_STATE, n_init=10):\n",
    "    X_arr = _prepare_X_for_clustering(X)\n",
    "    n_samples = X_arr.shape[0]\n",
    "\n",
    "    if k_min < 1:\n",
    "        k_min = 1\n",
    "    k_max = min(int(k_max), n_samples)\n",
    "    if k_max < k_min:\n",
    "        raise ValueError(\n",
    "            f\"Invalid k range after adjustment: k_min={k_min}, k_max={k_max}, n_samples={n_samples}\"\n",
    "        )\n",
    "\n",
    "    ks = list(range(k_min, k_max + 1))\n",
    "    inertias = []\n",
    "    for k in ks:\n",
    "        # Use integer n_init for wider sklearn compatibility\n",
    "        km = KMeans(n_clusters=k, n_init=n_init, random_state=random_state)\n",
    "        km.fit(X_arr)\n",
    "        inertias.append(float(km.inertia_))\n",
    "    return ks, inertias\n",
    "\n",
    "\n",
    "def choose_k_by_knee(ks, inertias):\n",
    "    if len(ks) == 0 or len(inertias) == 0:\n",
    "        raise ValueError(\"ks/inertias must be non-empty.\")\n",
    "    if len(ks) != len(inertias):\n",
    "        raise ValueError(\"ks and inertias must have the same length.\")\n",
    "\n",
    "    x = np.array(ks, dtype=float)\n",
    "    y = np.array(inertias, dtype=float)\n",
    "\n",
    "    p1 = np.array([x[0], y[0]])\n",
    "    p2 = np.array([x[-1], y[-1]])\n",
    "    line_vec = p2 - p1\n",
    "    denom = np.linalg.norm(line_vec)\n",
    "    if denom == 0.0:\n",
    "        # All inertias identical; fall back to first k\n",
    "        return ks[0], np.zeros_like(x)\n",
    "\n",
    "    # Compute perpendicular distances of all points to the line p1->p2\n",
    "    p1_to_points = np.vstack([x - p1[0], y - p1[1]]).T\n",
    "    distances = np.abs(np.cross(line_vec, p1_to_points) / denom)\n",
    "    knee_idx = int(np.argmax(distances))\n",
    "    return int(ks[knee_idx]), distances\n",
    "\n",
    "\n",
    "def auto_choose_k_and_fit(Xtr, random_state=42, n_init=10):\n",
    "    X_arr = _prepare_X_for_clustering(Xtr)\n",
    "    n_samples = X_arr.shape[0]\n",
    "\n",
    "    # Safer k_max heuristic\n",
    "    heuristic_max = max(3, n_samples // 50)  # ensure at least 3\n",
    "    k_max = min(15, heuristic_max, n_samples)  # never exceed n_samples\n",
    "    k_min = 1\n",
    "    ks, inertias = compute_elbow_inertia(\n",
    "        X_arr, k_min=k_min, k_max=k_max, random_state=random_state, n_init=n_init\n",
    "    )\n",
    "    best_k, _ = choose_k_by_knee(ks, inertias)\n",
    "\n",
    "    model = KMeans(n_clusters=best_k, n_init=n_init, random_state=random_state).fit(\n",
    "        X_arr\n",
    "    )\n",
    "    return {\n",
    "        \"best_k\": best_k,\n",
    "        \"ks\": ks,\n",
    "        \"inertias\": inertias,\n",
    "        \"labels\": model.labels_,\n",
    "        \"centers\": model.cluster_centers_,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "\n",
    "# Fallback if no features were selected by correlation: use top-variance features from the scaled data\n",
    "if isinstance(Xtr, pd.DataFrame) and Xtr.shape[1] == 0:\n",
    "    # pick up to 20 highest-variance features that exist in both train/test scaled frames\n",
    "    common_cols = list(X_train_scaled.columns.intersection(X_test_scaled.columns))\n",
    "    if len(common_cols) == 0:\n",
    "        raise ValueError(\n",
    "            \"No common numeric features available in train/test after preprocessing.\"\n",
    "        )\n",
    "    var_series = X_train_scaled[common_cols].var().sort_values(ascending=False)\n",
    "    fallback_n = min(\n",
    "        20, max(5, len(var_series))\n",
    "    )  # at least 5, up to 20 or all if fewer\n",
    "    selected_features = var_series.index[:fallback_n].tolist()\n",
    "    print(\n",
    "        f\"No features selected by correlation; falling back to {len(selected_features)} top-variance features.\"\n",
    "    )\n",
    "    Xtr = X_train_scaled[selected_features].copy()\n",
    "    Xte = X_test_scaled[selected_features].copy()\n",
    "\n",
    "# Use the robust auto-K routine with explicit parameters\n",
    "result = auto_choose_k_and_fit(Xtr, random_state=RANDOM_STATE, n_init=10)\n",
    "ks, inertias = result[\"ks\"], result[\"inertias\"]\n",
    "auto_k, distances = choose_k_by_knee(ks, inertias)\n",
    "\n",
    "print(\"Elbow Ks:\", ks)\n",
    "print(\"Inertias:\", [round(v, 2) for v in inertias])\n",
    "print(f\"Auto-selected K (knee): {auto_k}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(ks, inertias, marker=\"o\")\n",
    "plt.title(\"Elbow Method (Inertia vs K)\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Inertia (Within-Cluster SSE)\")\n",
    "for k, inertia, d in zip(ks, inertias, distances):\n",
    "    plt.annotate(\n",
    "        str(k),\n",
    "        (k, inertia),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 5),\n",
    "        ha=\"center\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "54c4fef2e5bced09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7) Train K-Means and map clusters to target means\n",
   "id": "e8c351c8c9011e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "K = int(auto_k)\n",
    "\n",
    "kmeans = KMeans(n_clusters=K, n_init=\"auto\", random_state=RANDOM_STATE)\n",
    "train_labels = kmeans.fit_predict(Xtr)\n",
    "\n",
    "# 1) Ensure y_train is a 1D numpy array\n",
    "y_train = np.asarray(y_train).reshape(-1)\n",
    "\n",
    "# 2) Filter out rows where y_train is NaN and align labels (and X if needed)\n",
    "# Assume these three arrays should be row-aligned\n",
    "# X_train: (N, d), y_train: (N), train_labels: (N)\n",
    "# Make sure they are all the same length first\n",
    "N = min(len(y_train), len(train_labels), len(X_train_scaled))\n",
    "X_train_scaled = X_train_scaled[:N]\n",
    "y_train = y_train[:N]\n",
    "train_labels = np.asarray(train_labels)[:N]\n",
    "\n",
    "# Build mask from the unfiltered labels array that defines validity\n",
    "valid_mask = ~np.isnan(y_train)\n",
    "\n",
    "# Apply the same mask to all aligned arrays\n",
    "X_train_scaled = X_train_scaled[valid_mask]\n",
    "y_train = y_train[valid_mask]\n",
    "train_labels = train_labels[valid_mask]\n",
    "\n",
    "\n",
    "# 3) Compute cluster means robustly on valid rows\n",
    "# Handle noise label (-1) if present by treating it as a separate \"cluster\"\n",
    "global_mean = float(np.mean(y_train))\n",
    "\n",
    "# Build a DataFrame to group by labels safely\n",
    "df_tmp = pd.DataFrame({\"label\": train_labels, \"y\": y_train})\n",
    "\n",
    "cluster_to_mean = df_tmp.groupby(\"label\", dropna=False)[\"y\"].mean()\n",
    "\n",
    "# 4) Replace any NaN cluster means with the global mean\n",
    "cluster_to_mean = cluster_to_mean.fillna(global_mean)\n",
    "\n",
    "# 5) Create a fast lookup for all possible non-negative labels\n",
    "max_label = (\n",
    "    int(cluster_to_mean.index[cluster_to_mean.index >= 0].max())\n",
    "    if np.any(cluster_to_mean.index >= 0)\n",
    "    else -1\n",
    ")\n",
    "means_array = np.full(max_label + 1 if max_label >= 0 else 0, global_mean, dtype=float)\n",
    "\n",
    "# Fill means for all non-negative labels\n",
    "for lbl, mean_val in cluster_to_mean.items():\n",
    "    if lbl >= 0:\n",
    "        means_array[lbl] = float(mean_val)\n",
    "\n",
    "# 6) Build predictions with safe handling for noise/unseen labels\n",
    "train_labels = np.asarray(train_labels, dtype=int)\n",
    "\n",
    "# Mask for noise labels (-1) or any labels outside max_label\n",
    "out_of_range_mask = (train_labels < 0) | (train_labels > max_label)\n",
    "in_range_mask = ~out_of_range_mask\n",
    "\n",
    "y_pred_train = np.empty_like(y_train, dtype=float)\n",
    "y_pred_train[out_of_range_mask] = global_mean  # fallback for -1 or unseen labels\n",
    "if max_label >= 0:\n",
    "    y_pred_train[in_range_mask] = means_array[train_labels[in_range_mask]]\n",
    "else:\n",
    "    # If there are no non-negative labels at all, everything uses global mean\n",
    "    y_pred_train[in_range_mask] = global_mean\n",
    "\n",
    "# 7) Compute metrics\n",
    "mae = mean_absolute_error(y_train, y_pred_train)\n",
    "rmse = mean_squared_error(y_train, y_pred_train) ** 0.5\n",
    "print(f\"In-sample MAE: {mae:.5f}, RMSE: {rmse:.5f}\")"
   ],
   "id": "489a96d7560a5d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8) Predict on test and build submission\n",
   "id": "c5f365983f808b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_labels = kmeans.predict(Xte)\n",
    "\n",
    "# If a cluster appears only in test (rare), fall back to global mean\n",
    "global_mean = y_train.mean()\n",
    "cluster_means_filled = cluster_to_mean.copy()\n",
    "# Ensure we have means for all clusters [K-1]\n",
    "for c in range(K):\n",
    "    if c not in cluster_means_filled.index:\n",
    "        cluster_means_filled.loc[c] = global_mean\n",
    "\n",
    "test_pred = cluster_means_filled.iloc[test_labels].values\n",
    "\n",
    "submission = pd.DataFrame({ID_COL: test_ids, TARGET_COL: test_pred})\n",
    "\n",
    "# Sort by ID if ID is sortable\n",
    "try:\n",
    "    submission = submission.sort_values(by=ID_COL)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "out_path = \"submission.csv\"\n",
    "submission.to_csv(out_path, index=False)\n",
    "print(f\"Saved submission to {out_path}\")"
   ],
   "id": "e1b8b2d3082a4db1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
